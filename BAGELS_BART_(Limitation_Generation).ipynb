{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Shb92PnqHaFB"
      },
      "outputs": [],
      "source": [
        "df2['input_text'] = df.apply(lambda row: f\"\"\"Abstract: {row['Abstract']}\n",
        "Introduction: {row['Introduction']}\n",
        "Experiment_and_Results: {row['Experiment_and_Results']}\n",
        "Conclusion: {row['Conclusion']}\n",
        "\"\"\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]\n",
        "!pip -q install transformers[torch] accelerate>=0.26.0"
      ],
      "metadata": {
        "id": "5zPO8VHiHkeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Assuming df2 already has the necessary columns\n",
        "# df2['input_text'] = df2['Abstract'] + ' ' + df2['Introduction'] + ' ' + df2['Conclusion']\n",
        "data = df2[['input_text', 'Limitation']]\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)  # 70% training, 30% testing\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data, max_len=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        source_enc = self.tokenizer(item['input_text'], truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
        "        target_enc = self.tokenizer(item['Limitation'], truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
        "\n",
        "        return {\n",
        "            'input_ids': source_enc['input_ids'].flatten(),\n",
        "            'attention_mask': source_enc['attention_mask'].flatten(),\n",
        "            'labels': target_enc['input_ids'].flatten()\n",
        "        }\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./bart_results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "train_dataset = TextDataset(tokenizer, train_data)\n",
        "test_dataset = TextDataset(tokenizer, test_data)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "bR1C49x-HkVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set device based on CUDA availability (if the code is in CPU and my env is in GPU then there will be a missmatch)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)  # Move model to the appropriate device\n",
        "\n",
        "def generate_limitation(text):\n",
        "    # Encode the input text and ensure tensor is on the correct device\n",
        "    inputs = tokenizer.encode(text, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
        "\n",
        "    # Generate output tokens\n",
        "    output_tokens = model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)\n",
        "\n",
        "    # Decode the output tokens to a string\n",
        "    limitation = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "    return limitation\n",
        "\n",
        "# Manually applying the function to ensure device consistency\n",
        "def apply_generate_limitations(df):\n",
        "    limitations = []\n",
        "    for text in df['input_text']:\n",
        "        limitation = generate_limitation(text)\n",
        "        limitations.append(limitation)\n",
        "    return limitations\n",
        "test_data['generated_limitations_bart'] = ''\n",
        "test_data['generated_limitations_bart'] = apply_generate_limitations(test_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "o9M9KpsYHkMN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}