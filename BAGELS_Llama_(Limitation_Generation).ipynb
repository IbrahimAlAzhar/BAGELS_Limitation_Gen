{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-nBwtG-L6WU"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "generated_limitations = []\n",
        "\n",
        "for i in range(len(df)): #\n",
        "    # Construct the prompt for generating limitations\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful, respectful, and honest assistant for generating limitations or shortcomings of a research paper.\n",
        "    I am providing 'Abstract', 'Introduction', 'Experiment_and_Results' and 'Conclusion' of a scientific paper.\n",
        "    Generate limitations based on these texts. \\n{df['response_string'][i]}\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty string to collect the limitation text\n",
        "    limitation_text = \"\"\n",
        "\n",
        "    # Generate the response using the Ollama chat model\n",
        "    stream = ollama.chat(\n",
        "        model='llama3.1:8b',\n",
        "        messages=[{'role': 'user', 'content': prompt}],\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    # Collect the response incrementally from the stream\n",
        "    for chunk in stream:\n",
        "        limitation_chunk = chunk.get('message', {}).get('content', '') or \"\"\n",
        "        limitation_text += limitation_chunk\n",
        "\n",
        "    # Append the collected limitation text to the list\n",
        "    generated_limitations.append(limitation_text)\n"
      ]
    }
  ]
}