{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "BERTScore"
      ],
      "metadata": {
        "id": "MHBQr7G0OfSi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nK2a9CkOZVZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "# Initialize the BERT scorer\n",
        "scorer = BERTScorer(model_type='roberta-large', lang=\"en\")\n",
        "\n",
        "# Function to calculate BERTScore for each row using one loop\n",
        "def calculate_bertscore(row):\n",
        "    # Calculate BERT Scores directly for the ground_truth and llm_generated of the row\n",
        "    _, _, F1 = scorer.score([row['ground_truth']], [row['llm_generated']])\n",
        "    return F1.mean().item()  # Return the mean F1 score\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "df_filtered['bert_score'] = df_filtered.apply(calculate_bertscore, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Update the function to handle lists in each row\n",
        "def extract_first_number_from_list(row):\n",
        "    for text in row:  # Iterate through each string in the list\n",
        "        match = re.match(r'^(\\d+)', text)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "    return None  # Return None if no number is found\n",
        "\n",
        "# Apply the updated function to the 'ground_truth' column\n",
        "df_filtered['section'] = df_filtered['ground_truth'].apply(extract_first_number_from_list)\n",
        "\n",
        "average_bert_score = df_filtered['bert_score'].mean()\n"
      ],
      "metadata": {
        "id": "JsNvk4PkOion"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROUGE score"
      ],
      "metadata": {
        "id": "kXM3tOd2OmyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Function to calculate similarity metrics for each row\n",
        "def calculate_metrics(row):\n",
        "    metrics = {}\n",
        "\n",
        "    # ROUGE scores\n",
        "    rouge_scores = rouge_scorer.score(row['ground_truth'], row['llm_generated'])\n",
        "    metrics['rouge1'] = rouge_scores['rouge1'].fmeasure\n",
        "    metrics['rouge2'] = rouge_scores['rouge2'].fmeasure\n",
        "    metrics['rougeL'] = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "    # Cosine Similarity\n",
        "    vectorizer = CountVectorizer().fit_transform([row['ground_truth'], row['llm_generated']])\n",
        "    vectors = vectorizer.toarray()\n",
        "    metrics['cosine_similarity'] = cosine_similarity(vectors)[0, 1]\n",
        "\n",
        "    # Jaccard Similarity\n",
        "    set1 = set(row['ground_truth'].split())\n",
        "    set2 = set(row['llm_generated'].split())\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    metrics['jaccard_similarity'] = intersection / union if union > 0 else 0\n",
        "\n",
        "    # BLEU Score\n",
        "    metrics['bleu_score'] = sentence_bleu([row['ground_truth'].split()], row['llm_generated'].split())\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Apply the function to each row in the DataFrame and store results in new columns\n",
        "metric_results = df_filtered.apply(calculate_metrics, axis=1)\n",
        "\n",
        "# Expand the dictionary into separate columns\n",
        "metric_results_df = pd.DataFrame(metric_results.tolist())\n",
        "df_filtered = pd.concat([df_filtered, metric_results_df], axis=1)\n"
      ],
      "metadata": {
        "id": "W6H6C36LOmsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average of each metric\n",
        "average_metrics = {\n",
        "    'Average ROUGE-1': df_filtered['rouge1'].mean(),\n",
        "    'Average ROUGE-2': df_filtered['rouge2'].mean(),\n",
        "    'Average ROUGE-L': df_filtered['rougeL'].mean(),\n",
        "    'Average Cosine Similarity': df_filtered['cosine_similarity'].mean(),\n",
        "    'Average Jaccard Similarity': df_filtered['jaccard_similarity'].mean(),\n",
        "    'Average BLEU Score': df_filtered['bleu_score'].mean()\n",
        "}\n",
        "\n",
        "# Print the average metrics\n",
        "average_metrics\n"
      ],
      "metadata": {
        "id": "OBuiNGiiOqYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic based evaluation"
      ],
      "metadata": {
        "id": "KBATY_v9Otbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "\n",
        "# Initialize KeyBERT model\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Function to extract keywords using KeyBERT\n",
        "def extract_keywords(text):\n",
        "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english')\n",
        "    return [kw[0] for kw in keywords]  # Extract just the keywords\n",
        "\n",
        "# Apply KeyBERT to 'ground_truth' and 'LLM_generated' columns\n",
        "df_filtered['ground_truth_words'] = df_filtered['ground_truth'].apply(extract_keywords)\n",
        "df_filtered['LLM_generated_words'] = df_filtered['llm_generated'].apply(extract_keywords)\n"
      ],
      "metadata": {
        "id": "bBmTQfHZOtVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Jaccard Similarity\n",
        "def jaccard_similarity(row):\n",
        "    set1 = set(row['ground_truth_words'])\n",
        "    set2 = set(row['LLM_generated_words'])\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "# Apply Jaccard Similarity to each row\n",
        "df_filtered['jaccard_similarity_topic'] = df_filtered.apply(jaccard_similarity, axis=1)\n",
        "df_filtered['jaccard_similarity_topic'].mean()"
      ],
      "metadata": {
        "id": "b6KPZTXROxLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to compute Cosine Similarity\n",
        "def cosine_sim(row):\n",
        "    vectorizer = CountVectorizer().fit_transform([' '.join(row['ground_truth_words']), ' '.join(row['LLM_generated_words'])])\n",
        "    vectors = vectorizer.toarray()\n",
        "    return cosine_similarity(vectors)[0, 1]\n",
        "\n",
        "# Apply Cosine Similarity to each row\n",
        "df_filtered['cosine_similarity_topic'] = df_filtered.apply(cosine_sim, axis=1)\n",
        "df_filtered['cosine_similarity_topic'].mean()\n"
      ],
      "metadata": {
        "id": "FLUKve2XOzXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to extract text between double asterisks\n",
        "def extract_text_between_asterisks(text):\n",
        "    matches = re.findall(r'\\*\\*(.*?)\\*\\*', text)\n",
        "    return matches\n",
        "\n",
        "# Apply the function to both columns and store results in new columns\n",
        "df_filtered['ground_truth_extracted'] = df_filtered['ground_truth'].apply(extract_text_between_asterisks)\n",
        "df_filtered['llm_generated_extracted'] = df_filtered['llm_generated'].apply(extract_text_between_asterisks)\n"
      ],
      "metadata": {
        "id": "JTHaWpc6O2Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTScore (heading)"
      ],
      "metadata": {
        "id": "PfavYzu5O4oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import BERTScorer\n",
        "\n",
        "# Initialize the BERT scorer\n",
        "scorer = BERTScorer(model_type='roberta-large', lang=\"en\")\n",
        "\n",
        "# Function to calculate BERTScore for each row\n",
        "def calculate_bertscore(row):\n",
        "    # Flatten the lists to single strings\n",
        "    ground_truth_text = ' '.join(row['ground_truth_extracted'])\n",
        "    llm_generated_text = ' '.join(row['llm_generated_extracted'])\n",
        "\n",
        "    # Calculate BERT Scores\n",
        "    _, _, F1 = scorer.score([ground_truth_text], [llm_generated_text])\n",
        "    return F1.mean().item()  # Return the mean F1 score\n",
        "\n",
        "# Apply the function to each row and store the results in a new column\n",
        "df_filtered['bert_score_heading'] = df_filtered.apply(calculate_bertscore, axis=1)\n"
      ],
      "metadata": {
        "id": "wRdHpBAoO4iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average of the 'bert_score' column in df_highest_score\n",
        "average_bert_score = df_filtered['bert_score_heading'].mean()\n"
      ],
      "metadata": {
        "id": "6mowcDcqO8xu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}