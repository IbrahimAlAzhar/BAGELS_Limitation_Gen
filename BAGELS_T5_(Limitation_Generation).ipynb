{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWQglqyAHwbj"
      },
      "outputs": [],
      "source": [
        "!pip -q install accelerate -U\n",
        "!pip -q install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# model = model.to(device)\n",
        "\n",
        "# df2['input_text'] = df2['Abstract'] + ' ' + df2['conclusion'] +  ' ' + df2['Introduction']\n",
        "# df2 = df2[['input_text', 'Limitation']]  # Focus on relevant columns\n",
        "\n",
        "data = df2[['input_text', 'Limitation']]\n",
        "\n",
        "train_df2, test_df2 = train_test_split(df2, test_size=0.3, random_state=42)\n",
        "\n",
        "class T5Dataset(Dataset):\n",
        "    def __init__(self, tokenizer, data, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        input_text = row['input_text']\n",
        "        target_text = row['Limitation']\n",
        "\n",
        "        source_encodings = self.tokenizer(\n",
        "            input_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        target_encodings = self.tokenizer(\n",
        "            target_text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': source_encodings['input_ids'].squeeze(0),  # remove batch dimension\n",
        "            'attention_mask': source_encodings['attention_mask'].squeeze(0),\n",
        "            'labels': target_encodings['input_ids'].squeeze(0)\n",
        "        }\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=300,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "train_dataset = T5Dataset(tokenizer, train_df2)\n",
        "test_dataset = T5Dataset(tokenizer, test_df2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# model.to('cpu')  # Move model to CPU for inference if not using a GPU\n",
        "\n",
        "# def generate_limitations(text):\n",
        "#     inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512)\n",
        "#     outputs = model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)\n",
        "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# test_df['predicted_limitation'] = test_df['input_text'].apply(generate_limitations)\n",
        "\n",
        "def generate_limitations(text):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512).to(device)\n",
        "    outputs = model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "test_df2['predicted_limitation'] = test_df2['input_text'].apply(generate_limitations)\n"
      ],
      "metadata": {
        "id": "TJe1vb7LJyLN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}