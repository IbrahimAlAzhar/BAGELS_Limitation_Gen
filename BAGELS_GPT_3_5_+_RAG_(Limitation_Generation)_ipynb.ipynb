{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index\n",
        "!pip install -q openai\n",
        "!pip install -q transformers\n",
        "!pip install -q accelerate\n",
        "!pip -q install llama-index-core\n",
        "!pip -q install llama-index-llms-openai\n",
        "!pip -q install llama-index-llms-replicate\n",
        "!pip -q install llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "id": "bHKYX6YlPshe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhlEw2QvPWEu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_rag_train = df2.sample(n=100, random_state=42)  # Use a fixed random state for reproducibility\n",
        "\n",
        "# Create the testing set by dropping the sampled rows from df2\n",
        "df2_rag_test = df2.drop(df_rag_train.index)\n",
        "\n",
        "# Now df_rag_train contains 100 random samples, and df2_test contains the rest\n",
        "df_rag_train.to_csv('df_rag_train.csv', index=False)\n",
        "df2_rag_test.to_csv('df2_rag_test.csv', index=False)\n",
        "\n",
        "from llama_index.core.llms import LLM\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "import os\n",
        "# from openai import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "documents = SimpleDirectoryReader(\"gdrive/My Drive/limitations_dataset/df_rag_train\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "# reset index\n",
        "df2_rag_test.reset_index(inplace=True, drop=False)\n",
        "\n",
        "df2_rag_test['response_string'] = df2_rag_test.apply(lambda row: f\"\"\"Abstract: {row['Abstract']}\n",
        "Introduction: {row['Introduction']}\n",
        "Methodology: {row['methodology']}\n",
        "Conclusion: {row['conclusion']}\n",
        "\"\"\", axis=1)\n",
        "\n",
        "index.storage_context.persist()\n",
        "\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "index = load_index_from_storage(storage_context=storage_context)\n",
        "from llama_index.core import ServiceContext, set_global_service_context\n",
        "from llama_index.llms import openai\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from IPython.display import Markdown, display\n",
        "# set global settings config\n",
        "\n",
        "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "Settings.llm = llm\n",
        "Settings.chunk_size = 512\n",
        "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=800, chunk_overlap=20)\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "system_prompt = '''You are a helpful, respectful, and honest assistant for generating limitations or shortcomings of a research paper.\n",
        " Generate limitations or shortcomings for the following passages from a scientific paper.\n",
        " I am providing Abstract, Introduction, Methodology, and Conclusion of a scientific paper.'''\n",
        "\n",
        "import io\n",
        "import sys\n",
        "\n",
        "generated_limitations = []\n",
        "\n",
        "for i in range(len(df)):  # Assuming you need to iterate from 11 to 11, which is effectively just once\n",
        "    query_engine = index.as_query_engine(\n",
        "        similarity_top_k=3,\n",
        "        streaming=True,\n",
        "    )\n",
        "    response = query_engine.query(\n",
        "        system_prompt + \": \" + df2_rag_test['response_string'][i]\n",
        "    )\n",
        "    limitation_text = \"\"\n",
        "    print(\"response is: \", response.print_response_stream())\n",
        "    # Redirect stdout to capture print outputs\n",
        "    old_stdout = sys.stdout  # Memorize the default stdout stream\n",
        "    sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "    try:\n",
        "        response.print_response_stream()\n",
        "        limitation_text = buffer.getvalue()  # Get whatever was printed to the \"fake\" stdout\n",
        "    finally:\n",
        "        sys.stdout = old_stdout  # Restore stdout. Important to do this early\n",
        "    print(\"limitation text: \",limitation_text.strip())\n",
        "    generated_limitations.append(limitation_text.strip())"
      ]
    }
  ]
}