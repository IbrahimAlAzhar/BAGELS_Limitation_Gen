{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgyBJwt1Ki1q"
      },
      "outputs": [],
      "source": [
        "# create segment using '\\n' in generated limitations\n",
        "\n",
        "def process_single_limitation(limitation_text):\n",
        "    # Split the text on newline characters to handle different blocks\n",
        "    blocks = limitation_text.split('\\n')\n",
        "    processed_blocks = []\n",
        "    for block in blocks:\n",
        "        # Split each block into sentences by '.'\n",
        "        sentences = [sentence.strip() for sentence in block.split('.') if sentence.strip()]\n",
        "        if sentences:  # Only add non-empty lists\n",
        "            processed_blocks.append(sentences)\n",
        "    return processed_blocks\n",
        "\n",
        "# Apply the function to each row in the 'Limitations' column\n",
        "df2['processed_limitations'] = df2['generated_limitations_mis_all'].apply(process_single_limitation)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add numbers to each list in a list of lists\n",
        "def add_numbers_to_list_of_lists(data):\n",
        "    numbered_list = []\n",
        "    number = 1  # Start numbering from 1\n",
        "    for inner_list in data:\n",
        "        # Add the number to the beginning of each list\n",
        "        numbered_list.append([f\"{number}.\"] + inner_list)\n",
        "        number += 1\n",
        "    return numbered_list\n",
        "\n",
        "# Apply the function to the 'processed_limitations' column\n",
        "df2['processed_limitations'] = df2['processed_limitations'].apply(add_numbers_to_list_of_lists)\n"
      ],
      "metadata": {
        "id": "KoBJe3WjKpSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_filter_text(lists):\n",
        "    # Process each sublist\n",
        "    cleaned_lists = []\n",
        "    for sublist in lists:\n",
        "        # Filter out empty strings and strip whitespace\n",
        "        cleaned_sublist = [s.strip() for s in sublist if s.strip()]\n",
        "        if cleaned_sublist:  # Only add non-empty sublists\n",
        "            cleaned_lists.append(cleaned_sublist)\n",
        "    return cleaned_lists\n",
        "\n",
        "df2['processed_limitations'] =df2['processed_limitations'].apply(clean_and_filter_text)\n"
      ],
      "metadata": {
        "id": "qjhOpSARKpK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert str to list\n",
        "import ast\n",
        "\n",
        "# Convert strings in the 'Lim_points_gt_gpt_lists' column to lists of lists\n",
        "df2['Lim_points_gt_gpt_lists'] = df2['Lim_points_gt_gpt_lists'].apply(ast.literal_eval)\n"
      ],
      "metadata": {
        "id": "h1ICypCCKpB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new column 'combined' with empty lists\n",
        "df2['combined'] = [[] for _ in range(len(df2))]\n",
        "\n",
        "# Nested loop to generate combinations without using product\n",
        "for i in range(len(df2)):\n",
        "    combined_list = []\n",
        "    list1 = df2['Lim_points_gt_gpt_lists'][i]\n",
        "    list2 = df2['processed_limitations'][i]\n",
        "\n",
        "    # Nested loop for combinations\n",
        "    for item1 in list1:\n",
        "        for item2 in list2:\n",
        "            #if 'Future' not in item1 and 'Future' not in item2:\n",
        "            combined_list.append((item1, item2))\n",
        "            # combined_list.append((item1, item2))\n",
        "\n",
        "    # Store the combinations in the 'combined' column for the current index\n",
        "    df2.at[i, 'combined'] = combined_list"
      ],
      "metadata": {
        "id": "gO8zLneqLBNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# at first, generate summary (index 0 to 412) (done)\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "all_generated_summary = []\n",
        "\n",
        "for i in range(len(df2)): # len(df2)\n",
        "    generated_summary = []\n",
        "    for description1, description2 in df2['combined'][i]:\n",
        "      prompt = '''Check whether 'list2' contains a topic or limitation from 'list1' or 'list1' contains a topic or limitation from 'list2'.\n",
        "      Your answer should be \"Yes\" or \"No\" \\n. List 1:''' + str(description1) + \"List2: \" + str(description2)\n",
        "      summary_text = \"\"  # Initialize an empty string to collect the limitation text\n",
        "      stream = client.chat.completions.create(\n",
        "          # model=\"gpt-3.5-turbo\",\n",
        "          model=\"gpt-4o-mini\",\n",
        "          messages=[\n",
        "              {\n",
        "                  \"role\": \"user\",\n",
        "                  \"content\": prompt,\n",
        "              }\n",
        "          ],\n",
        "          stream=True,\n",
        "          temperature=0  # Adjust the temperature as needed, max_tokens=150\n",
        "      )\n",
        "\n",
        "      for chunk in stream:\n",
        "          summary_chunk = chunk.choices[0].delta.content or \"\"\n",
        "          # print(limitation_chunk, end=\"\")\n",
        "          summary_text += summary_chunk  # Append each chunk to the limitation_text\n",
        "\n",
        "      # print(\"\\n\")  # Print a newline for readability\n",
        "      summary_chunks = []\n",
        "      summary_chunks.append(summary_text)\n",
        "\n",
        "      generated_summary.append((summary_chunks, \"list1\", description1, \"list2\", description2))\n",
        "    all_generated_summary.append(generated_summary)\n"
      ],
      "metadata": {
        "id": "1_4cnZURLBIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ground Truth Coverage"
      ],
      "metadata": {
        "id": "NH5cuvcFLMkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# process 2\n",
        "data = []\n",
        "row_num = 1  # Start row_num from 1, increment for each sublist\n",
        "\n",
        "# Extract data from nested_list2\n",
        "for sublist in all_generated_summary:\n",
        "    for is_match, list1_label, ground_truth, list2_label, llm_generated in sublist:\n",
        "        # Each tup is in the form of (list1, s1, s2, s3, s4)\n",
        "        # Append data to list as a dictionary to maintain column order\n",
        "        data.append({\n",
        "            'row_num': row_num,\n",
        "            'is_match': is_match[0],\n",
        "            'ground_truth': ground_truth,\n",
        "            'llm_generated': llm_generated\n",
        "        })\n",
        "    row_num += 1  # Increment row_num for each new sublist\n",
        "\n",
        "# Create DataFrame from the list of dictionaries\n",
        "df4 = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "-w09I2gBLBCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Update the function to handle lists in each row\n",
        "def extract_first_number_from_list(row):\n",
        "    for text in row:  # Iterate through each string in the list\n",
        "        match = re.match(r'^(\\d+)', text)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "    return None  # Return None if no number is found\n",
        "\n",
        "# Apply the updated function to the 'ground_truth' column\n",
        "df4['section'] = df4['ground_truth'].apply(extract_first_number_from_list)\n",
        "\n",
        "# Initialize variables\n",
        "current_section = None\n",
        "section_has_yes = False\n",
        "ck = 0\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for index, row in df4.iterrows():\n",
        "    # Check if we are still in the same section\n",
        "    if row['section'] == current_section:\n",
        "        # Check if there is a 'Yes' in 'is_match'\n",
        "        if row['is_match'] == 'Yes':\n",
        "            section_has_yes = True\n",
        "    else:\n",
        "        # We've reached a new section, check if the last section had a 'Yes'\n",
        "        if section_has_yes:\n",
        "            ck += 1\n",
        "        # Reset for new section\n",
        "        current_section = row['section']\n",
        "        section_has_yes = (row['is_match'] == 'Yes')\n",
        "\n",
        "# Check the last section after exiting the loop\n",
        "if section_has_yes:\n",
        "    ck += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "lsEd25-ILQCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total number of unique ground truth\n",
        "\n",
        "# Calculate consecutive blocks where 'ground_truth' is the same\n",
        "unique_blocks = df4['ground_truth'].ne(df4['ground_truth'].shift()).cumsum()\n",
        "\n",
        "# Group by these blocks and count each group\n",
        "group_counts = df4.groupby(unique_blocks)['ground_truth'].agg(['count'])\n",
        "\n",
        "# Output the results\n",
        "print(\"Number of unique consecutive 'ground_truth' texts and their counts:\")\n"
      ],
      "metadata": {
        "id": "H_orYUZ4LP73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ground Truth limitation coverage:\" ck*100/group_counts)"
      ],
      "metadata": {
        "id": "RyVRi2TNLP0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM Generated Coverage"
      ],
      "metadata": {
        "id": "xMhFhWKYLdl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_first_number(text):\n",
        "    import re\n",
        "    # Check if the input is a list\n",
        "    if isinstance(text, list):\n",
        "        # Join the list elements into a single string\n",
        "        text = \" \".join(text)\n",
        "    # Use regex to extract the first number\n",
        "    match = re.match(r'^(\\d+)', text)\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "# Apply the updated function to extract numbers\n",
        "df4['order'] = df4['llm_generated'].apply(extract_first_number)\n",
        "\n",
        "# Sort the DataFrame by 'row_num' and then by the extracted order\n",
        "df_recall = df4.sort_values(by=['row_num', 'order'])\n",
        "\n",
        "# Reset index for clean indices in the new DataFrame\n",
        "df_recall = df_recall.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ATeOB7g-LcsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder the columns by placing 'llm_generated' before 'ground_truth'\n",
        "df_recall = df_recall[['row_num', 'is_match', 'llm_generated', 'ground_truth', 'section', 'order']]\n"
      ],
      "metadata": {
        "id": "-AzkKck_LccI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how many\n",
        "# df_recall.drop('section', axis=1, inplace=True)\n",
        "import re\n",
        "\n",
        "def extract_first_number(text):\n",
        "    import re\n",
        "    # Check if the input is a list\n",
        "    if isinstance(text, list):\n",
        "        # Join the list elements into a single string\n",
        "        text = \" \".join(text)\n",
        "    # Use regex to extract the first number\n",
        "    match = re.match(r'^(\\d+)', text)\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "# Extract the first number and create a new column for it\n",
        "df_recall['section'] = df_recall['llm_generated'].apply(extract_first_number)\n",
        "\n",
        "# Initialize variables\n",
        "current_section = None\n",
        "section_has_yes = False\n",
        "ck = 0\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for index, row in df_recall.iterrows():\n",
        "    # Check if we are still in the same section\n",
        "    if row['section'] == current_section:\n",
        "        # Check if there is a 'Yes' in 'is_match'\n",
        "        if row['is_match'] == 'Yes':\n",
        "            section_has_yes = True\n",
        "    else:\n",
        "        # We've reached a new section, check if the last section had a 'Yes'\n",
        "        if section_has_yes:\n",
        "            ck += 1\n",
        "        # Reset for new section\n",
        "        current_section = row['section']\n",
        "        section_has_yes = (row['is_match'] == 'Yes')\n",
        "\n",
        "# Check the last section after exiting the loop\n",
        "if section_has_yes:\n",
        "    ck += 1\n",
        "\n",
        "print(\"Number of sections with at least one 'Yes':\", ck)"
      ],
      "metadata": {
        "id": "jJtDUhiqLi5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total number of unique ground truth\n",
        "\n",
        "# Calculate consecutive blocks where 'ground_truth' is the same\n",
        "unique_blocks = df_recall['llm_generated'].ne(df_recall['llm_generated'].shift()).cumsum()\n",
        "\n",
        "# Group by these blocks and count each group\n",
        "group_counts = df_recall.groupby(unique_blocks)['llm_generated'].agg(['count'])\n",
        "\n",
        "# Output the results\n",
        "print(\"Number of unique consecutive 'ground_truth' texts and their counts:\")\n",
        "print(group_counts)"
      ],
      "metadata": {
        "id": "GNVWjwdWLiyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LLM Generated limitation coverage:\" ck*100/group_counts)"
      ],
      "metadata": {
        "id": "_Z8eua4RLmUQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}