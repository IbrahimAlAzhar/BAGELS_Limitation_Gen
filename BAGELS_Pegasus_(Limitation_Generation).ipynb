{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v23I6qMhJ2tV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "data = df2[['input_text', 'Limitation']]\n",
        "\n",
        "train_df3, test_df3 = train_test_split(df2, test_size=0.7, random_state=42)\n",
        "\n",
        "class PegasusDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        source = self.tokenizer(row['input_text'], max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        target = self.tokenizer(row['Limitation'], max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        return {\n",
        "            'input_ids': source.input_ids.squeeze(0),\n",
        "            'attention_mask': source.attention_mask.squeeze(0),\n",
        "            'labels': target.input_ids.squeeze(0)\n",
        "        }\n",
        "\n",
        "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
        "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./pegasus_results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=300,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "train_dataset = PegasusDataset(tokenizer, train_df)\n",
        "test_dataset = PegasusDataset(tokenizer, test_df)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_limitation(text):\n",
        "    # Encode the input text and ensure tensor is on the correct device\n",
        "    inputs = tokenizer.encode(text, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
        "\n",
        "    # Generate output tokens\n",
        "    output_tokens = model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)\n",
        "\n",
        "    # Decode the output tokens to a string\n",
        "    limitation = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "    return limitation\n",
        "\n",
        "# Manually applying the function to ensure device consistency\n",
        "def apply_generate_limitations(df):\n",
        "    limitations = []\n",
        "    for text in df['input_text']:\n",
        "        limitation = generate_limitation(text)\n",
        "        limitations.append(limitation)\n",
        "    return limitations\n",
        "test_df3['predicted_limitation'] = test_df3['input_text'].apply(generate_limitation)"
      ],
      "metadata": {
        "id": "5xocD5zEKPwz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}